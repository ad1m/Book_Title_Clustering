<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Book Clustering</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
  </head>
  <body>

    <style>
    #right_div {
    float: right;
    border: 0px solid black;
    text-align: center;
}
</style>


    <section class="page-header">
      <br>
      <br>
      <h1 class="project-name">Clustering Book Titles</h1>
      <!--<h2 class="project-tagline">Part 1: Motivation, History, Theory, & Application</h2>-->
      <h3 class="project-tagline">By: Adam Lieberman</h3>
      <br>
      <br>
      <!--<a href="#" class="btn">View on GitHub</a>
      <a href="#" class="btn">Download .zip</a>
      <a href="#" class="btn">Download .tar.gz</a> -->
    </section>

    <section class="main-content">
    <h1>Background</h1>
    <p>I was recently given the following toy dataset:</p>
    <blockquote>
    	annotated hamlet published in<br>
		shakespeare's hamlet<br>
		harry potter and the chamber of secrets<br>
		harry potter and the sorcerer's stone first edition<br>
		romeo and juliet<br>
		a first edition romeo and juliet<br>
		shakespeare's romeo and juliet<br>
		to kill a mockingbird<br>
		1984<br>
		orwell's 1984<br>
		king lear<br>
		an old copy of 1984<br>
    </blockquote>
    <p>Above there are full book titles and fragments of book titles and the goal is to cluster the books that are similar e.g. cluster Harry Potter books from 1984 books from Romeo and Juilet books.</p>

    <h1>Libraries:</h1>
    <p>The following contains all libraries found in the code segments below:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">requests</span>
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">re</span>
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">bs4</span> <span style="color: #0000aa">import</span> BeautifulSoup
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">random</span>
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">numpy</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">np</span>
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">pandas</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">pd</span>

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">nltk</span>
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">nltk.corpus</span> <span style="color: #0000aa">import</span> stopwords

<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.feature_extraction.text</span> <span style="color: #0000aa">import</span> CountVectorizer
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.feature_extraction.text</span> <span style="color: #0000aa">import</span> TfidfTransformer
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.cluster</span> <span style="color: #0000aa">import</span> KMeans
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.decomposition</span> <span style="color: #0000aa">import</span> PCA
</pre></div>



    <h1>Data:</h1>
    <p>Initially I looked for a full dataset for this problem, but there did not seem to be a set of book titles with altered book titles dataset. So, I decided to create my own small dataset. To do so I first scraped the <a href="https://www.amazon.com/b?ie=UTF8&node=8192263011">100 recommended book titles to read</a> from Amazon using the requests package and BeautifulSoup. The code is as follows:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">link = <span style="color: #aa5500">&quot;https://www.amazon.com/b?ie=UTF8&amp;node=8192263011&quot;</span>
html = requests.get(link).text
bs = BeautifulSoup(html,<span style="color: #aa5500">&quot;html.parser&quot;</span>)
books = []
<span style="color: #0000aa">for</span> link <span style="color: #0000aa">in</span> bs.find_all(<span style="color: #aa5500">&quot;span&quot;</span>,{<span style="color: #aa5500">&quot;class&quot;</span>:<span style="color: #aa5500">&quot;a-size-small&quot;</span>}):
    books.append(link.text)
</pre></div>
<p>Now that we have 100 book titles, we can randomly choose a starting point for each book title and a random length for each book title and create 5 modified titles for each book title. Additionally, there are 14 books in Amazon's list which have less than or equal to 2 words, so we filter these out. We create our 5 modified book titles for each book as follows and keep lables according to which modified books belong in the same cluster as the original book title so that we can evaluate our clustering later on. We do so as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">book_labels = []
modified_labels = []
modified_books = []
lbl = <span style="color: #009999">0</span>
<span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> books: 
    token_book = i.split()
    word_len = <span style="color: #00aaaa">len</span>(token_book)
    book_labels.append(lbl)
    <span style="color: #aaaaaa; font-style: italic">#Filter out books of length &lt;= 2</span>
    <span style="color: #0000aa">if</span> <span style="color: #00aaaa">len</span>(token_book) &gt; <span style="color: #009999">2</span>:
        <span style="color: #0000aa">for</span> j <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">5</span>): 
            random_start = random.randint(<span style="color: #009999">0</span>,word_len-<span style="color: #009999">1</span>)
            random_word_len = random.randint(<span style="color: #009999">2</span>,word_len-<span style="color: #009999">1</span>)
            <span style="color: #0000aa">if</span> (random_start+random_word_len) &gt; <span style="color: #00aaaa">len</span>(token_book[random_start:]): 
                b_start = token_book[random_start:]
                b_end = token_book[<span style="color: #009999">0</span>:random_word_len - <span style="color: #00aaaa">len</span>(b_start)]
                b = b_start + b_end
            <span style="color: #0000aa">else</span>: 
                b = token_book[random_start:(random_start+random_word_len)]
            modified_labels.append(lbl)
            modified_books.append(<span style="color: #aa5500">&#39; &#39;</span>.join(b))
    lbl = lbl+<span style="color: #009999">1</span>
</pre></div>


		<p>After doing so we have 435 modified books and 100 regular book titles. We can now add the two lists together and create one list of all titles. We can additionally do the same for the cluster labels. We do so as follows:</p>
		<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_titles = books+modified_books
all_labels = book_labels+modified_labels
</pre></div>




	<p>We now have 535 book titles. We note that this is a small dataset.</p>  

	<h1>Preprocessing:</h1>
	<p>We now need to preprocess our data. We need our data to have a uniform format and be machine readable so we keep only letters and numbers (0-9), lowercase all letters, tokenize the words, and remove stop words (words that serve no importance). We will write a function called clean_data which takes in a book title and returns the cleaned book title. The function is as follows:</p>
	<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">clean_data</span>(raw_text): 
    letters = re.sub(<span style="color: #aa5500">&quot;[^a-zA-Z0-9]&quot;</span>, <span style="color: #aa5500">&quot; &quot;</span>,raw_text)                <span style="color: #aaaaaa; font-style: italic">#Keeps only letters and numbers</span>
    lower_case = letters.lower()                                  <span style="color: #aaaaaa; font-style: italic">#Keeps all words lowercase</span>
    tok = lower_case.split()                                      <span style="color: #aaaaaa; font-style: italic">#Tokenizes</span>
    stop_words = <span style="color: #00aaaa">set</span>(stopwords.words(<span style="color: #aa5500">&quot;english&quot;</span>))                  <span style="color: #aaaaaa; font-style: italic">#Create set of stopwords</span>
    wrds = [w <span style="color: #0000aa">for</span> w <span style="color: #0000aa">in</span> tok <span style="color: #0000aa">if</span> w <span style="color: #0000aa">not</span> <span style="color: #0000aa">in</span> stop_words]                <span style="color: #aaaaaa; font-style: italic">#Removes stopwords</span>
    <span style="color: #0000aa">return</span>(<span style="color: #aa5500">&quot; &quot;</span>.join(wrds))
</pre></div>


	<p>We can now create a pandas dataframe of our book titles and use an apply and lambda to clean each book title and then extract a numpy array of our cleaned book titles. We do so as follows:</p>
	<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #aaaaaa; font-style: italic">#Create pandas dataframe of all book titles</span>
df_data = pd.DataFrame(all_titles,columns=[<span style="color: #aa5500">&#39;Book_Title&#39;</span>])

<span style="color: #aaaaaa; font-style: italic">#Apply cleaning function on dataframe </span>
df_data[<span style="color: #aa5500">&quot;cleaned_data&quot;</span>] = df_data[<span style="color: #aa5500">&quot;Book_Title&quot;</span>].apply(<span style="color: #0000aa">lambda</span> i: clean_data(i))

<span style="color: #aaaaaa; font-style: italic">#Numpy array of cleaned data</span>
cleaned_data = df_data[<span style="color: #aa5500">&quot;cleaned_data&quot;</span>].values
</pre></div>

<p>We now have our preprocessed book titles:</p>

<h1>Feature Generation:</h1>
<p>Now that we have our cleaned book titles, we need to generate features for each book title. A popular feature is term frequency-inverse document frequency (tf-idf). Tfidf is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We can caluclate the term frequency of a term, $t$, as 
$$TF(t) = \frac{\text{the number of times}\; t \; \text{appears in a document}}{\text{total number of terms in the document}}$$</p>
<p>We can calculate the inverse document frequency as: 
$$IDF(t) = \text{log}\left(\frac{\text{Total number of documents}}{\text{Number of documents with the term}\; t \; \text{in it}}\right)$$</p>
<p>For example, say we have a document that has 100 words and the word "dog" appears 3 times. The term frequenct for dog, $TF(\text{dog}) = \frac{3}{100} = 0.03$. Now, say we have 10,000,000 documents and the word "dog" appears in 1,000 of these. Then the inverse docment frequency $IDF(\text{dog}) = \text{log}\left(\frac{10,000,000}{1,000}\right) = 4$. Now, the tfidf score can be calculated as $0.03*4 = 0.12$.</p>
<p>There are some pros and cons of using tf-idf which we highlight below:</p>
<ul>
<li>Pros:</li>
<ol>
<li>Easy to compute</li>
<li>Have a basic metric to extract the most descriptive terms of a document</li>
<li>Can easily compute the similarity between 2 documents</li>
<li>Great for nearly identical documents</li>
</ol>
<li>Cons:</li>
<ol>
<li>Does not capture position in text</li>
<li>Does not capture semantics</li>
<li>Documents that do not have a majority of similar words can perform poorly</li>
</ol>
</ul>
<p>Since our documents contain similar words, we will carry on forward using tf-idf feature vectors. We can use sklearn to create a count vectorizer and fit our data to it. We can then create a tf-idf transformer and fit the counts to it to obtain our tf-idf features. We do so as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">vectorizer = CountVectorizer(analyzer=<span style="color: #aa5500">&quot;word&quot;</span>,tokenizer=<span style="color: #00aaaa">None</span>,preprocessor=<span style="color: #00aaaa">None</span>,stop_words=<span style="color: #00aaaa">None</span>)
vec_features = vectorizer.fit_transform(cleaned_data)  <span style="color: #aaaaaa; font-style: italic">#The counts</span>
X_counts = vec_features.toarray()
transformer = TfidfTransformer(smooth_idf=<span style="color: #00aaaa">False</span>)   <span style="color: #aaaaaa; font-style: italic">#Create transformer</span>
X_tfidf = transformer.fit_transform(X_counts)     <span style="color: #aaaaaa; font-style: italic">#Fit transformer on the counts </span>
X_tfidf = X_tfidf.toarray()   
</pre></div>
<p>We now have our tf-idf feature vectors for all of our book titles.</p>
<h1>K-Means Clustering:</h1>
<p>Now that we have our features we need to cluster them. KMeans clustering is an unsupervised learning algorithm that partitions $n$ samples into $k$ clusters. We randomly create $k$ centroids, one for each cluster. These centroids are the mean locations of our clusters. We then take each point in the data set and associate it to the nearest centroid. Once all points are assigned we can recalculate the positions of the $k$ centroids. We then re assign all the points and repeat the process of updating the centroid position until no points move from the previous clustering assignment to the next. This produces a separation of the objects into groups from which the metric to be minimized can be calculated.</p>
<p>We can visually see this process as follows:</p>
<center><img src="images/kmeans_example.gif"></center> 
<p>We see that at each iteration the stars, representing the centroid for each cluster, are adjusted and new data is shifted into the clusters.</p>
<p>
<p>Using sklearn we can easily instantiate a KMeans object and then fit our data to it as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">k = <span style="color: #009999">100</span>     <span style="color: #aaaaaa; font-style: italic">#1 cluster for each book </span>
km = KMeans(n_clusters=k, init=<span style="color: #aa5500">&#39;k-means++&#39;</span>, max_iter=<span style="color: #009999">100</span>)
km.fit(X_tfidf)
</pre></div>
<p>We can now create a cluster map so that we can print out the book titles assigned to each cluster. To do so we create a pandas dataframe and then let one column have the book titles and the other column have the cluster numbers, which we can obtain using the .labels_ function from sklearn. We do so as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">cluster_map = pd.DataFrame()
cluster_map[<span style="color: #aa5500">&#39;data_index&#39;</span>] = <span style="color: #00aaaa">list</span>(df_data[<span style="color: #aa5500">&quot;Book_Title&quot;</span>].values)
cluster_map[<span style="color: #aa5500">&#39;cluster&#39;</span>] = km.labels_
</pre></div>
<p>We can now print out the first 4 rows in our dataframe:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">print</span>(cluster_map.head(<span style="color: #009999">4</span>))

&gt;&gt;&gt;
                              data_index  cluster
<span style="color: #009999">0</span>                 <span style="color: #009999">1984</span> (Signet Classics)       <span style="color: #009999">34</span>
<span style="color: #009999">1</span>                A Brief History of Time        <span style="color: #009999">2</span>
<span style="color: #009999">2</span>  A Heartbreaking Work of Staggering...       <span style="color: #009999">74</span>
<span style="color: #009999">3</span>   A Long Way Gone: Memoirs of a Boy...       <span style="color: #009999">66</span>
</pre></div>

<p>We see each title has a corresponding cluster.</p>

<p>We can now print the following book titles per cluster using the following code:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">100</span>):
    <span style="color: #0000aa">print</span>(<span style="color: #aa5500">&quot;Cluster&quot;</span>, i,<span style="color: #aa5500">&quot;: \n&quot;</span>,cluster_map[cluster_map.cluster == i],<span style="color: #aa5500">&quot;\n&quot;</span>)
</pre></div>

<p>Let us look at cluster 64. This cluster should contain all book titles similar to "The Golden Compass: His Dark Materials". Below are the book titles in this cluster:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">Cluster <span style="color: #009999">64</span> : 
                                  data_index  cluster
<span style="color: #009999">65</span>   The Golden Compass: His Dark Materials       <span style="color: #009999">64</span>
<span style="color: #009999">375</span>                Golden Compass: His Dark       <span style="color: #009999">64</span>
<span style="color: #009999">376</span>                      Dark Materials The       <span style="color: #009999">64</span>
<span style="color: #009999">378</span>                      Dark Materials The       <span style="color: #009999">64</span>
<span style="color: #009999">379</span>                      His Dark Materials       <span style="color: #009999">64</span> 
</pre></div>

<p>We see that this cluster contains books that are similar to the book title.</p>

<p>Let us look at cluster 5. This cluster should contain all book titles similar to "Harry Potter and the Sorcerer's Stone". The results are as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">Cluster <span style="color: #009999">5</span> : 
                                 data_index  cluster
<span style="color: #009999">28</span>   Harry Potter <span style="color:">and</span> the Sorcerer<span style="color:">&#39;s Stone        <span style="color: #009999">5</span>
<span style="color: #009999">210</span>                 Sorcerer<span style="color:">&#39;s Stone Harry        <span style="color: #009999">5</span>
<span style="color: #009999">211</span>        Potter <span style="color">and</span> the Sorcerer<span style="color:">&#39;s Stone        <span style="color: #009999">5</span>
<span style="color: #009999">212</span>                            Stone Harry        <span style="color: #009999">5</span>
<span style="color: #009999">213</span>        Harry Potter <span style="color:">and</span> the Sorcerer<span style="color:">&#39;s        <span style="color: #009999">5</span>
<span style="color: #009999">214</span>             the Sorcerer<span style="color:">&#39;s Stone Harry        <span style="color: #009999">5 </span>
</pre></div>

<h1>Performance - Purity:</h1>
<p>By examining the results by eye, we see that we did a pretty good job. The items in the clusters make sense as we see a lot of similar book titles clustered together. However, let us use a clustering performance metric called purity to quantitatively evaluate our clustering. To compute purity, each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by $N$. Mathematically speaking, 
$$\text{purity}\left(A,B\right) = \frac{1}{N}\sum_k \max_j |w_k \cap c_j|$$ 
where $A = {w_1,w_2,...,w_k}$ is the set of clusters and $B = {c_1,c_2,...,c_j}$ is the set of classes.</p> 
</p>
<p>Let us look at the following example to better understand purity:</p>
<center><img src="images/purity_example.png" style="width:500px"></center>
<p>In the above figure the x belongs to cluster 1, the circle belongs to cluster 2, and the diamond belongs to cluster 3. We see that we had 5 x's correctly classified in cluster 1 so $5 = max_j | w_1 \cap c_j|$. In cluster 2 we had 4 circles correctly classified so $4 = max_j |w_2 \cap c_j|$. In cluster 3 we had 3 diamond's correctly classified so $3 = \max_j | w_3 \cap c_j|$. We can count all of the x's, circles, and diamonds and we see that $N = 17$. Thus, purity = $\frac{1}{17} \left(5+4+3 \right) \approx 0.71.$</p>
<p>Let us now write the following python function to compute purity:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">purity_score</span>(clusters, classes):
    
    A = np.c_[(clusters,classes)]

    n_accurate = <span style="color: #009999">0.</span>

    <span style="color: #0000aa">for</span> j <span style="color: #0000aa">in</span> np.unique(A[:,<span style="color: #009999">0</span>]):
        z = A[A[:,<span style="color: #009999">0</span>] == j, <span style="color: #009999">1</span>]
        x = np.argmax(np.bincount(z))
        n_accurate += <span style="color: #00aaaa">len</span>(z[z == x])

    <span style="color: #0000aa">return</span> n_accurate / A.shape[<span style="color: #009999">0</span>]
</pre></div>
<p>We can now pass in the clusters from the KMeans algorithm and our labels (classes that we previously stored) and print our purity:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">print</span>(<span style="color: #aa5500">&quot;Purity: &quot;</span>,purity_score(km.labels_,all_labels))

&gt;&gt;&gt;
Purity:  <span style="color: #009999">0.9102803738317757</span>
</pre></div>

<p>We see that our purity was ~0.91. If our clustering algoright did a perfect job our purity would be 1.0. Our purity value is very high and is a good indicator that our clustering did a good job.</p>

<h1>Conclusion:</h1>
<p>Looking across all 100 book title clusters we see that KMeans with tf-idf features worked well. This highlights the case that if we have to cluster short text documents where there are some nearly identical words in each document then using tf-idf with KMeans is not a problem. If for instance, we had 140 character tweets, tf-idf and KMeans might not give us great clustering as there may not be many words in common across tweets clustered based on user.</p>

<h1>Code:</h1>
<p>All code found in this tutorial can be found <a href="https://github.com/ad1m/Book_Title_Clustering">here</a>.</p>



    </section>

  </body>
</html>
