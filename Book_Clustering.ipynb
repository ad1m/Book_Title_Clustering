{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Scrape 80 Amazon Recommended Books to Read:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = \"https://www.amazon.com/b?ie=UTF8&node=8192263011\"\n",
    "html = requests.get(link).text\n",
    "bs = BeautifulSoup(html,\"html.parser\")\n",
    "books = []\n",
    "for link in bs.find_all(\"span\",{\"class\":\"a-size-small\"}):\n",
    "    books.append(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984 (Signet Classics)', 'A Brief History of Time', 'A Heartbreaking Work of Staggering...', 'A Long Way Gone: Memoirs of a Boy...', 'The Bad Beginning: Or, Orphans!', 'A Wrinkle in Time', 'Selected Stories, 1968-1994', \"Alice's Adventures in Wonderland...\", \"All the President's Men\", \"Angela's Ashes: A Memoir\"]\n"
     ]
    }
   ],
   "source": [
    "print(books[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generate 5 Modified Books for each Book:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modified_books = []\n",
    "for i in books: \n",
    "    token_book = i.split()\n",
    "    word_len = len(token_book)\n",
    "    if len(token_book) != 1:\n",
    "        for j in range(5): \n",
    "            random_start = random.randint(0,word_len-1)\n",
    "            random_word_len = random.randint(1,word_len-1)\n",
    "            modified_books.append(' '.join(token_book[random_start:random_start+random_word_len])) #tokenized to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Put Together Our Clustering Dataset:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984 (Signet Classics)', 'Diary of a Wimpy Kid, Book 1', 'Me Talk Pretty One Day', 'The Corrections: A Novel', 'The Poisonwood Bible: A Novel']\n"
     ]
    }
   ],
   "source": [
    "all_titles = books+modified_books\n",
    "print(all_titles[0:100:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pre-process Our Data:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(raw_text): \n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \",raw_text)                   #Keeps only letters\n",
    "    lower_case = letters.lower()                                  #Keeps all words lowercase\n",
    "    tok = lower_case.split()                                      #Tokenizes\n",
    "    stop_words = set(stopwords.words(\"english\"))                  #Create set of stopwords\n",
    "    wrds = [w for w in tok if w not in stop_words]                #Removes stopwords\n",
    "    return(\" \".join(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Book_Title\n",
      "0                 1984 (Signet Classics)\n",
      "1                A Brief History of Time\n",
      "2  A Heartbreaking Work of Staggering...\n",
      "3   A Long Way Gone: Memoirs of a Boy...\n",
      "4        The Bad Beginning: Or, Orphans!\n",
      "5                      A Wrinkle in Time\n",
      "6            Selected Stories, 1968-1994\n",
      "7    Alice's Adventures in Wonderland...\n",
      "8                All the President's Men\n",
      "9               Angela's Ashes: A Memoir\n",
      "\n",
      " ['signet classics' 'brief history time' 'heartbreaking work staggering'\n",
      " 'long way gone memoirs boy' 'bad beginning orphans']\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.DataFrame(all_titles,columns=['Book_Title'])\n",
    "print(df_data.head(10))\n",
    "\n",
    "#Apply cleaning function on dataframe \n",
    "df_data[\"cleaned_data\"] = df_data[\"Book_Title\"].apply(lambda i: clean_data(i))\n",
    "\n",
    "#numpy array of cleaned data\n",
    "cleaned_data = df_data[\"cleaned_data\"].values\n",
    "print(\"\\n\",cleaned_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generate tf-idf Features:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None,max_features=200)\n",
    "vec_features = vectorizer.fit_transform(cleaned_data)  #The counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_counts = vec_features.toarray()\n",
    "transformer = TfidfTransformer(smooth_idf=False)   #Create transformer\n",
    "X_tfidf = transformer.fit_transform(X_counts)     #Fit transformer on the counts \n",
    "X_tfidf = X_tfidf.toarray()                       #numpy array of the tf-idf features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create K-Means Object:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=80, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 80 #1 cluster for each book \n",
    "km = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "km.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = list(cleaned_data)\n",
    "cluster_map['cluster'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             data_index  cluster\n",
      "0                       signet classics       72\n",
      "1                    brief history time       45\n",
      "2         heartbreaking work staggering        0\n",
      "3             long way gone memoirs boy       74\n",
      "4                 bad beginning orphans       38\n",
      "5                          wrinkle time       11\n",
      "6                      selected stories       27\n",
      "7           alice adventures wonderland       78\n",
      "8                         president men       28\n",
      "9                   angela ashes memoir        0\n",
      "10                         god margaret        0\n",
      "11                          bel canto p        0\n",
      "12                              beloved        0\n",
      "13                born run hidden tribe       51\n",
      "14   breath eyes memory oprah book club        2\n",
      "15         catch th anniversary edition       73\n",
      "16            charlie chocolate factory        0\n",
      "17                        charlotte web       71\n",
      "18                        cutting stone       39\n",
      "19               daring greatly courage        0\n",
      "20                 diary wimpy kid book       59\n",
      "21            dune dune chronicles book       47\n",
      "22                           fahrenheit       50\n",
      "23              fear loathing las vegas        0\n",
      "24                            gone girl       74\n",
      "25                       goodnight moon       21\n",
      "26                   great expectations       37\n",
      "27               guns germs steel fates       33\n",
      "28          harry potter sorcerer stone       39\n",
      "29                           cold blood       62\n",
      "..                                  ...      ...\n",
      "565                               apart        0\n",
      "566                         things fall       18\n",
      "567                              things       18\n",
      "568                          fall apart        0\n",
      "569                              things       18\n",
      "570                         mockingbird        0\n",
      "571                                kill       43\n",
      "572                                kill       43\n",
      "573                                kill       43\n",
      "574                         mockingbird        0\n",
      "575                                 war        0\n",
      "576                                            0\n",
      "577                        war ii story        0\n",
      "578                                            0\n",
      "579                               story        0\n",
      "580                                            0\n",
      "581                               dolls        0\n",
      "582                               dolls        0\n",
      "583                              valley        0\n",
      "584                                            0\n",
      "585                               poems        0\n",
      "586                       sidewalk ends       49\n",
      "587                 sidewalk ends poems       49\n",
      "588                       sidewalk ends       49\n",
      "589                                            0\n",
      "590                         wild things       63\n",
      "591                         wild things       63\n",
      "592                              things       18\n",
      "593                                wild       63\n",
      "594                         wild things       63\n",
      "\n",
      "[595 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Some Sample Cluster Values:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: \n",
      "        data_index  cluster\n",
      "66   great gatsby        1\n",
      "226         great        1\n",
      "227         great        1\n",
      "425         great        1\n",
      "427         great        1\n",
      "428         great        1\n",
      "429  great gatsby        1 \n",
      "\n",
      "Cluster 3: \n",
      "                         data_index  cluster\n",
      "44   human bondage bantam classics        3\n",
      "315           human bondage bantam        3\n",
      "316                  human bondage        3\n",
      "317  human bondage bantam classics        3\n",
      "318           human bondage bantam        3\n",
      "319                  human bondage        3 \n",
      "\n",
      "Cluster 5: \n",
      "                                data_index  cluster\n",
      "33   kitchen confidential updated edition        5\n",
      "260                       updated edition        5\n",
      "261                       updated edition        5\n",
      "262          confidential updated edition        5 \n",
      "\n",
      "Cluster 39: \n",
      "                       data_index  cluster\n",
      "18                 cutting stone       39\n",
      "28   harry potter sorcerer stone       39\n",
      "187                        stone       39\n",
      "188                        stone       39\n",
      "236               sorcerer stone       39\n",
      "237                     sorcerer       39\n",
      "239               sorcerer stone       39 \n",
      "\n",
      "Cluster 51: \n",
      "                 data_index  cluster\n",
      "13   born run hidden tribe       51\n",
      "160           hidden tribe       51\n",
      "161                    run       51\n",
      "162       run hidden tribe       51\n",
      "163                    run       51\n",
      "164                  tribe       51 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster 1: \\n\",cluster_map[cluster_map.cluster == 1],\"\\n\")\n",
    "print(\"Cluster 3: \\n\",cluster_map[cluster_map.cluster == 3],\"\\n\")\n",
    "print(\"Cluster 5: \\n\",cluster_map[cluster_map.cluster == 5],\"\\n\")\n",
    "print(\"Cluster 39: \\n\",cluster_map[cluster_map.cluster == 39],\"\\n\")\n",
    "print(\"Cluster 51: \\n\",cluster_map[cluster_map.cluster == 51],\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
